{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "410cb33b-cd53-4705-bde3-ca88c9232c62",
   "metadata": {},
   "source": [
    "# Laboratorio 6 - GPUs con Python\n",
    "**alumno09:** Laura Llamas López"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3431319e-2e6b-43c1-a2f6-61ccba592c21",
   "metadata": {},
   "source": [
    "## Evaluating a vectorial function on CPU and GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d142183-b6bb-42e7-ba9d-35bf0f91dc0c",
   "metadata": {},
   "source": [
    "### CPU: plain and numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b92dd336-9997-4323-80e4-f285e9cc2db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "8.13 ms ± 29.9 μs per loop (mean ± std. dev. of 2 runs, 5 loops each)\n",
      "18.4 ms ± 599 ns per loop (mean ± std. dev. of 2 runs, 5 loops each)\n",
      "18.5 ms ± 53.8 μs per loop (mean ± std. dev. of 2 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numba import njit, jit\n",
    "\n",
    "# Python plain implementation w/ numba \n",
    "@njit\n",
    "def grade2_vector(x, y, a, b, c):\n",
    "    z = np.zeros(x.size)\n",
    "    for i in range(x.size):\n",
    "        z[i] = a*x[i]*x[i] + b*y[i] + c\n",
    "    return z\n",
    "\n",
    "# Numpy ufunc\n",
    "def grade2_ufunc(x, y, a, b, c):\n",
    "    return a*x**2 + b*y + c\n",
    "\n",
    "# size of the vectors\n",
    "size = 5_000_000\n",
    "\n",
    "# allocating and populating the vectors\n",
    "a_cpu = np.random.rand(size)\n",
    "b_cpu = np.random.rand(size)\n",
    "c_cpu = np.zeros(size)\n",
    "\n",
    "a = 3.5\n",
    "b = 2.8\n",
    "c = 10\n",
    "\n",
    "# Printing input values\n",
    "#print(a_cpu)\n",
    "#print(b_cpu)\n",
    "# Random function in Numpy always use float64\n",
    "print(a_cpu.dtype)\n",
    "\n",
    "c_cpu = grade2_vector(a_cpu, b_cpu, a, b, c)\n",
    "\n",
    "\n",
    "# Evaluating the time\n",
    "\n",
    "# Numba Python: huge improvement, better that numpy code\n",
    "%timeit -n 5 -r 2 grade2_vector(a_cpu, b_cpu, a, b, c)\n",
    "\n",
    "# w/ a numpy ufunc manually coded\n",
    "%timeit -n 5 -r 2 grade2_ufunc(a_cpu, b_cpu, a, b, c)\n",
    "\n",
    "# using the general numpy ufunc \n",
    "%timeit -n 5 -r 2 a*a_cpu**2 + b*b_cpu + c\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff336b38-d6ec-411c-bcf3-a51bfdd7dd8e",
   "metadata": {},
   "source": [
    "## a) Librería CuPy: Aceleración con GPU\n",
    "\n",
    "En esta sección utilizamos CuPy para acelerar el cálculo usando la GPU. CuPy es una librería muy similar a NumPy pero diseñada específicamente para GPUs. La mayoría de funciones de NumPy tienen el mismo nombre en CuPy.\n",
    "\n",
    "Vamos a medir el tiempo en dos escenarios:\n",
    "1. **Con copia de datos**: Creamos los arrays en CPU (NumPy) y los copiamos a la GPU\n",
    "2. **Sin copia de datos**: Creamos los arrays directamente en la GPU usando CuPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16621753-88a2-4924-b9c0-e8c5d9030d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CASO 1: Con copia de datos (CPU -> GPU)\n",
      "======================================================================\n",
      "Tiempo con copia: 20.703 ms\n",
      "\n",
      "======================================================================\n",
      "CASO 2: Sin copia de datos (arrays creados directamente en GPU)\n",
      "======================================================================\n",
      "Tiempo sin copia: 4.415 ms\n",
      "\n",
      "======================================================================\n",
      "RESUMEN - CuPy con GPU\n",
      "======================================================================\n",
      "Con copia de datos: 20.703 ms\n",
      "Sin copia de datos: 4.415 ms\n",
      "Overhead de copia:  16.287 ms\n",
      "Factor: 4.69x más lento con copias\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "from cupyx.profiler import benchmark\n",
    "\n",
    "# =============================================================================\n",
    "# CASO 1: CON COPIA DE DATOS (CPU -> GPU)\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CASO 1: Con copia de datos (CPU -> GPU)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Función que incluye las copias CPU<->GPU\n",
    "def grade2_with_copy(x_cpu, y_cpu, a, b, c):\n",
    "    x_gpu = cp.asarray(x_cpu)  # copiar a GPU\n",
    "    y_gpu = cp.asarray(y_cpu)  \n",
    "    z_gpu = a * x_gpu**2 + b * y_gpu + c  # calcular en GPU\n",
    "    z_cpu = cp.asnumpy(z_gpu)  # copiar resultado a CPU\n",
    "    return z_cpu\n",
    "\n",
    "# Medir tiempo con benchmark\n",
    "exec_with_copy = benchmark(grade2_with_copy, (a_cpu, b_cpu, a, b, c), \n",
    "                           n_repeat=10, n_warmup=2)\n",
    "time_with_copy = np.average(exec_with_copy.gpu_times) * 1e3\n",
    "print(f\"Tiempo con copia: {time_with_copy:.3f} ms\")\n",
    "\n",
    "# =============================================================================\n",
    "# CASO 2: SIN COPIA DE DATOS (creación directa en GPU)\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CASO 2: Sin copia de datos (arrays creados directamente en GPU)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Crear arrays directamente en GPU\n",
    "x_gpu = cp.random.rand(size)\n",
    "y_gpu = cp.random.rand(size)\n",
    "\n",
    "# Calcular usando la ufunc ya definida (funciona con CuPy arrays)\n",
    "z_gpu = grade2_ufunc(x_gpu, y_gpu, a, b, c)\n",
    "\n",
    "# Medir tiempo con benchmark (sin copias)\n",
    "exec_no_copy = benchmark(grade2_ufunc, (x_gpu, y_gpu, a, b, c), \n",
    "                         n_repeat=10, n_warmup=2)\n",
    "time_no_copy = np.average(exec_no_copy.gpu_times) * 1e3\n",
    "print(f\"Tiempo sin copia: {time_no_copy:.3f} ms\")\n",
    "\n",
    "# =============================================================================\n",
    "# RESUMEN\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESUMEN - CuPy con GPU\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Con copia de datos: {time_with_copy:.3f} ms\")\n",
    "print(f\"Sin copia de datos: {time_no_copy:.3f} ms\")\n",
    "print(f\"Overhead de copia:  {time_with_copy - time_no_copy:.3f} ms\")\n",
    "print(f\"Factor: {time_with_copy / time_no_copy:.2f}x más lento con copias\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e42c483-3c63-4c20-bb60-d60524c19ddc",
   "metadata": {},
   "source": [
    "## b) Librería Numba: Creación de ufunc para GPU\n",
    "\n",
    "En esta sección utilizamos Numba con el decorador `@vectorize` para crear una ufunc que se ejecuta directamente en la GPU. A diferencia de CuPy que es una librería completa, Numba nos permite convertir funciones Python en ufuncs optimizadas para GPU.\n",
    "\n",
    "El decorador `@vectorize` con `target='cuda'` genera automáticamente código CUDA que se ejecuta en paralelo en la GPU. No necesitamos paralelizar explícitamente porque la GPU ya maneja la paralelización internamente.\n",
    "\n",
    "Mediremos el tiempo en dos escenarios:\n",
    "1. **Con copia automática**: Pasamos arrays de NumPy directamente y Numba gestiona las copias CPU↔GPU automáticamente\n",
    "2. **Sin copia**: Copiamos manualmente los arrays a GPU antes de medir para excluir el overhead de transferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b757c2c0-b4fe-4f85-9fe2-b89442e9a98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CASO 1: Con copia automática (Numba gestiona CPU <-> GPU)\n",
      "======================================================================\n",
      "Benchmark (CuPy):     18.471 ms\n",
      "timing_numba_GPU:     18.871 ms\n",
      "timing_numba_GPU2:    18.159 ms\n",
      "\n",
      "======================================================================\n",
      "CASO 2: Sin copia (arrays copiados manualmente a GPU)\n",
      "======================================================================\n",
      "Benchmark (CuPy):     2.189 ms\n",
      "timing_numba_GPU:     2.093 ms\n",
      "timing_numba_GPU2:    1.985 ms\n",
      "\n",
      "======================================================================\n",
      "RESUMEN - Numba con GPU (promedio de los 3 métodos)\n",
      "======================================================================\n",
      "Con copia automática: 18.500 ms\n",
      "Sin copia:            2.089 ms\n",
      "Overhead de copia:    16.411 ms\n",
      "Factor: 8.86x más lento con copias\n"
     ]
    }
   ],
   "source": [
    "from numba import vectorize\n",
    "from numba import cuda\n",
    "from cupyx.profiler import benchmark\n",
    "import time\n",
    "\n",
    "# Definir funciones de timing para Numba GPU\n",
    "def timing_numba_GPU(func, args, n_repeat=10, n_warmup=1):\n",
    "    \"\"\"Timing function based on numba cuda.synchronize()\"\"\"\n",
    "    for i in range(n_warmup):\n",
    "        out = func(*args)\n",
    "    cuda.synchronize()\n",
    "    timing = np.empty(n_repeat)\n",
    "    for i in range(timing.size):\n",
    "        tic = time.time()\n",
    "        out = func(*args)\n",
    "        cuda.synchronize()\n",
    "        toc = time.time()\n",
    "        timing[i] = toc - tic\n",
    "    return timing.mean()*1e3\n",
    "\n",
    "def timing_numba_GPU2(func, args, n_repeat=10, n_warmup=1):\n",
    "    \"\"\"Timing function based on cupy Events\"\"\"\n",
    "    import cupy as cp\n",
    "    gpu_start = cp.cuda.Event()\n",
    "    gpu_end = cp.cuda.Event()\n",
    "    for i in range(n_warmup):\n",
    "        out = func(*args)\n",
    "    gpu_start.record()\n",
    "    for i in range(n_repeat):\n",
    "        out = func(*args)\n",
    "    gpu_end.record()\n",
    "    gpu_end.synchronize()\n",
    "    t_gpu = cp.cuda.get_elapsed_time(gpu_start, gpu_end)\n",
    "    return t_gpu / n_repeat\n",
    "\n",
    "# Crear ufunc con @vectorize para GPU\n",
    "@vectorize(['float64(float64, float64, float64, float64, float64)'], target='cuda')\n",
    "def grade2_numba_gpu(x, y, a, b, c):\n",
    "    return a * x**2 + b * y + c\n",
    "\n",
    "# =============================================================================\n",
    "# CASO 1: CON COPIA AUTOMÁTICA (CPU -> GPU automático)\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CASO 1: Con copia automática (Numba gestiona CPU <-> GPU)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Método 1: benchmark de CuPy\n",
    "exec_benchmark = benchmark(grade2_numba_gpu, (a_cpu, b_cpu, a, b, c), \n",
    "                          n_repeat=10, n_warmup=2)\n",
    "time_benchmark = np.average(exec_benchmark.gpu_times) * 1e3\n",
    "print(f\"Benchmark (CuPy):     {time_benchmark:.3f} ms\")\n",
    "\n",
    "# Método 2: timing_numba_GPU (cuda.synchronize)\n",
    "time_numba1 = timing_numba_GPU(grade2_numba_gpu, (a_cpu, b_cpu, a, b, c), \n",
    "                               n_repeat=10, n_warmup=2)\n",
    "print(f\"timing_numba_GPU:     {time_numba1:.3f} ms\")\n",
    "\n",
    "# Método 3: timing_numba_GPU2 (CuPy Events)\n",
    "time_numba2 = timing_numba_GPU2(grade2_numba_gpu, (a_cpu, b_cpu, a, b, c), \n",
    "                                n_repeat=10, n_warmup=2)\n",
    "print(f\"timing_numba_GPU2:    {time_numba2:.3f} ms\")\n",
    "\n",
    "# =============================================================================\n",
    "# CASO 2: SIN COPIA (arrays ya en GPU)\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CASO 2: Sin copia (arrays copiados manualmente a GPU)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Copiar arrays a GPU manualmente ANTES de medir (usando CuPy como en apartado a)\n",
    "x_gpu_numba = cp.asarray(a_cpu)\n",
    "y_gpu_numba = cp.asarray(b_cpu)\n",
    "\n",
    "# Método 1: benchmark de CuPy\n",
    "exec_benchmark_no_copy = benchmark(grade2_numba_gpu, (x_gpu_numba, y_gpu_numba, a, b, c), \n",
    "                                   n_repeat=10, n_warmup=2)\n",
    "time_benchmark_no_copy = np.average(exec_benchmark_no_copy.gpu_times) * 1e3\n",
    "print(f\"Benchmark (CuPy):     {time_benchmark_no_copy:.3f} ms\")\n",
    "\n",
    "# Método 2: timing_numba_GPU (cuda.synchronize)\n",
    "time_numba1_no_copy = timing_numba_GPU(grade2_numba_gpu, (x_gpu_numba, y_gpu_numba, a, b, c), \n",
    "                                       n_repeat=10, n_warmup=2)\n",
    "print(f\"timing_numba_GPU:     {time_numba1_no_copy:.3f} ms\")\n",
    "\n",
    "# Método 3: timing_numba_GPU2 (CuPy Events)\n",
    "time_numba2_no_copy = timing_numba_GPU2(grade2_numba_gpu, (x_gpu_numba, y_gpu_numba, a, b, c), \n",
    "                                        n_repeat=10, n_warmup=2)\n",
    "print(f\"timing_numba_GPU2:    {time_numba2_no_copy:.3f} ms\")\n",
    "\n",
    "# =============================================================================\n",
    "# RESUMEN\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESUMEN - Numba con GPU (promedio de los 3 métodos)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calcular promedio de los 3 métodos\n",
    "time_with_copy_avg = (time_benchmark + time_numba1 + time_numba2) / 3\n",
    "time_no_copy_avg = (time_benchmark_no_copy + time_numba1_no_copy + time_numba2_no_copy) / 3\n",
    "\n",
    "print(f\"Con copia automática: {time_with_copy_avg:.3f} ms\")\n",
    "print(f\"Sin copia:            {time_no_copy_avg:.3f} ms\")\n",
    "print(f\"Overhead de copia:    {time_with_copy_avg - time_no_copy_avg:.3f} ms\")\n",
    "print(f\"Factor: {time_with_copy_avg / time_no_copy_avg:.2f}x más lento con copias\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c96b0c-6fe1-4774-85c7-470ac3a781a8",
   "metadata": {},
   "source": [
    "## c) Interpretación de resultados\n",
    "\n",
    "#### Tabla 1: Resultados en CPU (baseline)\n",
    "\n",
    "| Método | Tiempo | Observaciones |\n",
    "|--------|--------|---------------|\n",
    "| **Numba (@njit)** | 8.13 ms | Mejor rendimiento en CPU |\n",
    "| **NumPy ufunc** | 18.4 ms | 2.26x más lento que Numba |\n",
    "\n",
    "#### Tabla 2: Resultados en GPU con CuPy\n",
    "\n",
    "| Escenario | Tiempo | Speedup vs Numba CPU | Overhead de copia |\n",
    "|-----------|--------|----------------------|-------------------|\n",
    "| **Con copia de datos** | 20.70 ms | 0.39x (2.55x más lento) | 16.29 ms |\n",
    "| **Sin copia de datos** | 4.42 ms | **1.84x más rápido** | - |\n",
    "\n",
    "#### Tabla 3: Resultados en GPU con Numba @vectorize\n",
    "\n",
    "| Escenario | Tiempo | Speedup vs Numba CPU | Overhead de copia |\n",
    "|-----------|--------|----------------------|-------------------|\n",
    "| **Con copia automática** | 18.50 ms | 0.44x (2.28x más lento) | 16.41 ms |\n",
    "| **Sin copia** | 2.09 ms | **3.89x más rápido** | - |\n",
    "\n",
    "### Análisis de resultados\n",
    "\n",
    "Los experimentos muestran diferencias importantes entre las dos librerías de GPU y el impacto crítico de las transferencias de memoria.\n",
    "\n",
    "Cuando comparamos **CuPy** y **Numba** sin copias de datos, Numba obtiene tiempos notablemente mejores (2.09 ms frente a 4.42 ms de CuPy), logrando una aceleración de 3.89x respecto a Numba CPU. Esto probablemente se debe a que Numba genera código CUDA específico para la función mediante el decorador @vectorize, mientras que CuPy ejecuta operaciones más genéricas sobre arrays en GPU. CuPy, aunque más lento que Numba, sigue siendo competitivo con un speedup de 1.84x.\n",
    "\n",
    "El **overhead de las transferencias CPU↔GPU** es prácticamente idéntico en ambas librerías (16.29 ms para CuPy y 16.41 ms para Numba), lo cual tiene sentido porque ambas utilizan el mismo hardware y protocolo de comunicación. Este overhead domina completamente el tiempo de ejecución cuando incluimos las copias: tanto CuPy (20.70 ms) como Numba (18.50 ms) resultan más lentas que NumPy en CPU (18.4 ms), y mucho más lentas que Numba CPU (8.13 ms).\n",
    "\n",
    "En conclusión, usar la GPU solo tiene sentido cuando los datos ya están en memoria GPU o cuando realizamos suficientes operaciones consecutivas para amortizar el costo de transferencia. Para un único cálculo vectorial como el de este ejercicio, Numba CPU (@njit) sigue siendo competitiva por su simplicidad y buen rendimiento. Sin embargo, si trabajamos con pipelines de múltiples operaciones sobre los mismos datos, Numba GPU sería la opción más rápida, seguida de CuPy GPU."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
