{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8861764",
   "metadata": {},
   "source": [
    "# Laboratorio 6 - GPUs con Python\n",
    "**alumno09:** Laura Llamas López\n",
    "\n",
    "### Numpy Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8019c56a-e5ce-469b-a406-9c6056ab9c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.06 s ± 5.51 ms per loop (mean ± std. dev. of 2 runs, 1 loop each)\n",
      "Result shape: (7000, 7000)\n",
      "Result type: float32\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example: Large matrices (adjust size as needed)\n",
    "n = 7000  # For very large matrices, ensure you have enough RAM\n",
    "A = np.random.rand(n, n).astype(np.float32)\n",
    "B = np.random.rand(n, n).astype(np.float32)\n",
    "\n",
    "C = np.dot(A, B)  # warm-up and Matrix multiplication\n",
    "\n",
    "%timeit -r 2 -o np.dot(A, B)\n",
    "\n",
    "print(f\"Result shape: {C.shape}\")\n",
    "print(f\"Result type: {C.dtype}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f9fac1",
   "metadata": {},
   "source": [
    "### Pytorch code con GPU\n",
    "\n",
    "Utilizamos Pytorch para realizar la multiplicación de matrices en GPU. Pytorch está optimizado para operaciones de Deep Learning y aprovecha automáticamente la GPU cuando está disponible.\n",
    "\n",
    "Mediremos el tiempo en dos escenarios:\n",
    "1. **Con transferencia de datos**: Matrices generadas en CPU (NumPy) y copiadas a GPU\n",
    "2. **Sin transferencia**: Matrices generadas directamente en GPU usando Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6824f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "# Verificar si hay GPU disponible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# CASO 1: CON TRANSFERENCIA DE DATOS (NumPy -> GPU)\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CASO 1: Con transferencia de datos (CPU -> GPU)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def matmul_pytorch_with_transfer(A_np, B_np):\n",
    "    \"\"\"Convierte arrays NumPy a tensores Pytorch en GPU, multiplica, y devuelve a CPU\"\"\"\n",
    "    A_gpu = torch.from_numpy(A_np).to(device)  # CPU -> GPU\n",
    "    B_gpu = torch.from_numpy(B_np).to(device)\n",
    "    C_gpu = torch.matmul(A_gpu, B_gpu)         # Multiplicación en GPU\n",
    "    C_cpu = C_gpu.cpu().numpy()                # GPU -> CPU\n",
    "    return C_cpu\n",
    "\n",
    "# Warmup\n",
    "_ = matmul_pytorch_with_transfer(A, B)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "# Medir tiempo\n",
    "times_with_transfer = []\n",
    "for _ in range(5):\n",
    "    start = time.time()\n",
    "    C_torch_transfer = matmul_pytorch_with_transfer(A, B)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "    times_with_transfer.append(end - start)\n",
    "\n",
    "time_with_transfer = np.mean(times_with_transfer) * 1e3\n",
    "print(f\"Tiempo con transferencia: {time_with_transfer:.3f} ms\")\n",
    "print(f\"Result shape: {C_torch_transfer.shape}\")\n",
    "print(f\"Result type: {C_torch_transfer.dtype}\")\n",
    "\n",
    "# =============================================================================\n",
    "# CASO 2: SIN TRANSFERENCIA (generación directa en GPU)\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CASO 2: Sin transferencia (matrices creadas directamente en GPU)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Crear matrices directamente en GPU\n",
    "A_gpu = torch.rand(n, n, dtype=torch.float32, device=device)\n",
    "B_gpu = torch.rand(n, n, dtype=torch.float32, device=device)\n",
    "\n",
    "# Warmup\n",
    "C_gpu = torch.matmul(A_gpu, B_gpu)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "# Medir tiempo\n",
    "times_no_transfer = []\n",
    "for _ in range(5):\n",
    "    start = time.time()\n",
    "    C_gpu = torch.matmul(A_gpu, B_gpu)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "    times_no_transfer.append(end - start)\n",
    "\n",
    "time_no_transfer = np.mean(times_no_transfer) * 1e3\n",
    "print(f\"Tiempo sin transferencia: {time_no_transfer:.3f} ms\")\n",
    "print(f\"Result shape: {C_gpu.shape}\")\n",
    "print(f\"Result type: {C_gpu.dtype}\")\n",
    "\n",
    "# =============================================================================\n",
    "# RESUMEN\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESUMEN - Pytorch con GPU\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Con transferencia de datos: {time_with_transfer:.3f} ms\")\n",
    "print(f\"Sin transferencia:          {time_no_transfer:.3f} ms\")\n",
    "if time_with_transfer > time_no_transfer:\n",
    "    print(f\"Overhead de transferencia:  {time_with_transfer - time_no_transfer:.3f} ms\")\n",
    "    print(f\"Factor: {time_with_transfer / time_no_transfer:.2f}x más lento con transferencia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64493a9e",
   "metadata": {},
   "source": [
    "## Resultados de la ejecución en bohr-gpu\n",
    "\n",
    "### Salida de la ejecución\n",
    "\n",
    "```\n",
    "========================================\n",
    "Ejecutando matrix-mult-alumno09.ipynb\n",
    "========================================\n",
    "580 ms ± 1.03 ms per loop (mean ± std. dev. of 2 runs, 1 loop each)\n",
    "Result shape: (7000, 7000)\n",
    "Result type: float32\n",
    "Usando dispositivo: cuda\n",
    "GPU: NVIDIA GeForce RTX 2080 Ti\n",
    "\n",
    "======================================================================\n",
    "CASO 1: Con transferencia de datos (CPU -> GPU)\n",
    "======================================================================\n",
    "Tiempo con transferencia: 106.532 ms\n",
    "Result shape: (7000, 7000)\n",
    "Result type: float32\n",
    "\n",
    "======================================================================\n",
    "CASO 2: Sin transferencia (matrices creadas directamente en GPU)\n",
    "======================================================================\n",
    "Tiempo sin transferencia: 50.373 ms\n",
    "Result shape: torch.Size([7000, 7000])\n",
    "Result type: torch.float32\n",
    "\n",
    "======================================================================\n",
    "RESUMEN - Pytorch con GPU\n",
    "======================================================================\n",
    "Con transferencia de datos: 106.532 ms\n",
    "Sin transferencia:          50.373 ms\n",
    "Overhead de transferencia:  56.159 ms\n",
    "Factor: 2.11x más lento con transferencia\n",
    "```\n",
    "\n",
    "### Tabla comparativa de resultados\n",
    "\n",
    "| Método | Tiempo | Speedup vs NumPy CPU |\n",
    "|--------|--------|----------------------|\n",
    "| **NumPy CPU** | 580 ms | 1.0x (baseline) |\n",
    "| **Pytorch GPU con transferencia** | 106.5 ms | **5.45x más rápido** |\n",
    "| **Pytorch GPU sin transferencia** | 50.4 ms | **11.51x más rápido** |\n",
    "\n",
    "### Análisis de resultados\n",
    "\n",
    "**Nota**: Los resultados muestran `torch.Size([7000, 7000])` en lugar de la tupla `(7000, 7000)` de NumPy porque Pytorch utiliza su propio tipo para representar las dimensiones de los tensores. Ambos representan lo mismo: una matriz cuadrada de 7000×7000.\n",
    "\n",
    "Los resultados de la multiplicación de matrices 7000×7000 en bohr demuestran la enorme potencia de Pytorch para operaciones matriciales intensivas.\n",
    "\n",
    "**Pytorch sin transferencia** logra un speedup de 11.51x sobre NumPy CPU, reduciendo el tiempo de 580 ms a solo 50.4 ms. Este rendimiento excepcional se debe a que la multiplicación de matrices es una operación fundamental en Deep Learning y está extremadamente optimizada en Pytorch mediante el uso de bibliotecas BLAS optimizadas para GPU (cuBLAS). Además, al generar las matrices directamente en GPU con `torch.rand(..., device='cuda')`, evitamos completamente las transferencias de memoria.\n",
    "\n",
    "**Pytorch con transferencia** sigue ofreciendo un speedup significativo de 5.45x (106.5 ms), a pesar de incluir el costo de convertir los arrays NumPy a tensores Pytorch y copiarlos a GPU. El **overhead de transferencia** es de 56.2 ms, lo cual representa un factor de 2.11x de ralentización. Este overhead incluye tres operaciones: convertir dos matrices NumPy (7000×7000 cada una, ~196 MB en total de float32) a tensores Pytorch, copiarlas a memoria GPU, y devolver el resultado a CPU. A pesar de este costo considerable, la operación completa con transferencias sigue siendo mucho más rápida que NumPy CPU.\n",
    "\n",
    "En conclusión, Pytorch es excepcionalmente eficiente para multiplicación de matrices en GPU, especialmente cuando los datos ya residen en memoria GPU o cuando el tamaño del problema es lo suficientemente grande para que el cómputo compense el overhead de transferencia. Este rendimiento explica por qué Pytorch es la librería dominante en Deep Learning, donde las operaciones matriciales masivas son ubicuas en redes neuronales."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13",
   "language": "python",
   "name": "python313"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
